{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c1fda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98daa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac330b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dublin.csv\n",
      "Downloaded dublinbikes_20180701_20181001.csv\n",
      "Downloaded dublinbikes_20181001_20190101.csv\n",
      "Downloaded dublinbikes_20190101_20190401.csv\n",
      "Downloaded dublinbikes_20190401_20190701.csv\n",
      "Downloaded dublinbikes_20190701_20191001.csv\n",
      "Downloaded dublinbikes_20191001_20200101.csv\n",
      "Downloaded dublinbikes_20200101_20200401.csv\n",
      "Downloaded dublinbikes_20200401_20200701.csv\n",
      "Downloaded dublinbikes_20200701_20201001.csv\n",
      "Downloaded dublinbikes_20201001_20210101.csv\n",
      "Downloaded dublinbikes_20210101_20210401.csv\n",
      "Downloaded dublinbikes_20210401_20210701.csv\n",
      "Downloaded dublinbikes_20210701_20211001.csv\n",
      "Downloaded dublinbikes_20211001_20220101.csv\n",
      "Downloaded dublinbike-historical-data-2021-10.csv\n",
      "Downloaded dublinbike-historical-data-2021-11.csv\n",
      "Downloaded dublinbike-historical-data-2021-12.csv\n",
      "Downloaded dublinbike-historical-data-2022-01.csv\n",
      "Downloaded dublinbike-historical-data-2022-02.csv\n",
      "Downloaded dublinbike-historical-data-2022-03.csv\n",
      "Downloaded dublinbike-historical-data-2022-04.csv\n",
      "Downloaded dublinbike-historical-data-2022-05.csv\n",
      "Downloaded dublinbike-historical-data-2022-06.csv\n",
      "Downloaded dublinbike-historical-data-2022-07.csv\n",
      "Downloaded dublinbike-historical-data-2022-08.csv\n",
      "Downloaded dublinbike-historical-data-2022-09.csv\n",
      "Downloaded dublinbike-historical-data-2022-10.csv\n",
      "Downloaded dublinbike-historical-data-2022-11.csv\n",
      "Downloaded dublinbike-historical-data-2022-12.csv\n",
      "Downloaded dublinbike-historical-data-2023-01.csv\n",
      "Downloaded dublinbike-historical-data-2023-02.csv\n",
      "Downloaded dublinbike-historical-data-2023-03.csv\n",
      "Downloaded dublinbike-historical-data-2023-04.csv\n",
      "Downloaded dublinbike-historical-data-2023-05.csv\n",
      "Downloaded dublinbike-historical-data-2023-06.csv\n",
      "Downloaded dublinbike-historical-data-2023-07.csv\n",
      "Downloaded dublinbike-historical-data-2023-08.csv\n",
      "Downloaded dublinbike-historical-data-2023-09.csv\n",
      "Downloaded dublinbike-historical-data-2023-10.csv\n",
      "Downloaded dublinbike-historical-data-2023-11.csv\n",
      "Downloaded dublinbike-historical-data-2023-12.csv\n",
      "Downloaded dublinbike-historical-data-2024-01.csv\n"
     ]
    }
   ],
   "source": [
    "url = 'https://data.gov.ie/dataset/dublinbikes-api'\n",
    "\n",
    "# Send a request to the webpage URL\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # This will raise an error if the request failed\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all links to CSV files\n",
    "csv_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.csv')]\n",
    "\n",
    "# Download each CSV file\n",
    "for link in csv_links:\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Extract the filename from the link\n",
    "    filename = os.path.basename(link)\n",
    "\n",
    "    # Save the file\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    print(f'Downloaded {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec3c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    'dublinbike-historical-data-2023-01.csv',\n",
    "    'dublinbike-historical-data-2023-02.csv',\n",
    "    'dublinbike-historical-data-2023-03.csv',\n",
    "    'dublinbike-historical-data-2023-04.csv',\n",
    "    'dublinbike-historical-data-2023-05.csv',\n",
    "    'dublinbike-historical-data-2023-06.csv',\n",
    "    'dublinbike-historical-data-2023-07.csv',\n",
    "    'dublinbike-historical-data-2023-08.csv',\n",
    "    'dublinbike-historical-data-2023-09.csv',\n",
    "    'dublinbike-historical-data-2023-10.csv',\n",
    "    'dublinbike-historical-data-2023-11.csv',\n",
    "    'dublinbike-historical-data-2023-12.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe03f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading dublinbike-historical-data-2023-01.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-02.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-03.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-04.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-05.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-06.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-07.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-08.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-09.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-10.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-11.csv: name 'pd' is not defined\n",
      "Error reading dublinbike-historical-data-2023-12.csv: name 'pd' is not defined\n",
      "No dataframes to concatenate.\n"
     ]
    }
   ],
   "source": [
    "dataframes = []\n",
    "for file_name in file_names:\n",
    "    file_path = file_name\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames into one and save to a new CSV file\n",
    "if dataframes:\n",
    "    dublin_data = pd.concat(dataframes, ignore_index=True)\n",
    "    output_file = 'dublin_data_combined.csv'\n",
    "    dublin_data.to_csv(output_file, index=False)\n",
    "    output_file\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb4f8f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/raphaelbyrne/anaconda3/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42337edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://cycling.data.tfl.gov.uk/usage-stats/350JourneyDataExtract26Dec2022-01Jan2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/351JourneyDataExtract02Jan2023-08Jan2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/352JourneyDataExtract09Jan2023-15Jan2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/353JourneyDataExtract16Jan2023-22Jan2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/354JourneyDataExtract23Jan2023-29Jan2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/355JourneyDataExtract30Jan2023-05Feb2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/356JourneyDataExtract06Feb2023-12Feb2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/357JourneyDataExtract13Feb2023-19Feb2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/358JourneyDataExtract20Feb2023-26Feb2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/359JourneyDataExtract27Feb2023-05Mar2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/360JourneyDataExtract06Mar2023-12Mar2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/361JourneyDataExtract13Mar2023-19Mar2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/362JourneyDataExtract20Mar2023-26Mar2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/363JourneyDataExtract27Mar2023-02Apr2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/364JourneyDataExtract03Apr2023-09Apr2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/365JourneyDataExtract10Apr2023-16Apr2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/366JourneyDataExtract17Apr2023-23Apr2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/367JourneyDataExtract24Apr2023-30Apr2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/368JourneyDataExtract01May2023-07May2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/369JourneyDataExtract08May2023-14May2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/370JourneyDataExtract15May2023-21May2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/371JourneyDataExtract22May2023-28May2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/372JourneyDataExtract29May2023-04Jun2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/373JourneyDataExtract05Jun2023-11Jun2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/374JourneyDataExtract12Jun2023-18Jun2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/375JourneyDataExtract19Jun2023-30Jun2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/376JourneyDataExtract01Jul2023-14Jul2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/377JourneyDataExtract15Jul2023-31Jul2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/378JourneyDataExtract01Aug2023-14Aug2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/378JourneyDataExtract15Aug2023-31Aug2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/379JourneyDataExtract01Sep2023-14Sep2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/380JourneyDataExtract15Sep2023-30Sep2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/381JourneyDataExtract01Oct2023-14Oct2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/382JourneyDataExtract15Oct2023-31Oct2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/383JourneyDataExtract01Nov2023-14Nov2023.csv', 'http://cycling.data.tfl.gov.uk/usage-stats/384JourneyDataExtract15Nov2023-30Nov2023.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "# Reading the list of file names from a CSV file\n",
    "with open('london_bikes_urls.csv', 'r') as f:\n",
    "    csv_list = f.read().splitlines()\n",
    "\n",
    "# Base URL for downloading the files\n",
    "website = 'http://cycling.data.tfl.gov.uk/usage-stats/'\n",
    "\n",
    "# Generating the complete URLs without using urllib.parse.quote()\n",
    "url_list = [website + x for x in csv_list]\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f97dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 350JourneyDataExtract26Dec2022-01Jan2023.csv\n",
      "Downloaded 351JourneyDataExtract02Jan2023-08Jan2023.csv\n",
      "Downloaded 352JourneyDataExtract09Jan2023-15Jan2023.csv\n",
      "Downloaded 353JourneyDataExtract16Jan2023-22Jan2023.csv\n",
      "Downloaded 354JourneyDataExtract23Jan2023-29Jan2023.csv\n",
      "Downloaded 355JourneyDataExtract30Jan2023-05Feb2023.csv\n",
      "Downloaded 356JourneyDataExtract06Feb2023-12Feb2023.csv\n",
      "Downloaded 357JourneyDataExtract13Feb2023-19Feb2023.csv\n",
      "Downloaded 358JourneyDataExtract20Feb2023-26Feb2023.csv\n",
      "Downloaded 359JourneyDataExtract27Feb2023-05Mar2023.csv\n",
      "Downloaded 360JourneyDataExtract06Mar2023-12Mar2023.csv\n",
      "Downloaded 361JourneyDataExtract13Mar2023-19Mar2023.csv\n",
      "Downloaded 362JourneyDataExtract20Mar2023-26Mar2023.csv\n",
      "Downloaded 363JourneyDataExtract27Mar2023-02Apr2023.csv\n",
      "Downloaded 364JourneyDataExtract03Apr2023-09Apr2023.csv\n",
      "Downloaded 365JourneyDataExtract10Apr2023-16Apr2023.csv\n",
      "Downloaded 366JourneyDataExtract17Apr2023-23Apr2023.csv\n",
      "Downloaded 367JourneyDataExtract24Apr2023-30Apr2023.csv\n",
      "Downloaded 368JourneyDataExtract01May2023-07May2023.csv\n",
      "Downloaded 369JourneyDataExtract08May2023-14May2023.csv\n",
      "Downloaded 370JourneyDataExtract15May2023-21May2023.csv\n",
      "Downloaded 371JourneyDataExtract22May2023-28May2023.csv\n",
      "Downloaded 372JourneyDataExtract29May2023-04Jun2023.csv\n",
      "Downloaded 373JourneyDataExtract05Jun2023-11Jun2023.csv\n",
      "Downloaded 374JourneyDataExtract12Jun2023-18Jun2023.csv\n",
      "Downloaded 375JourneyDataExtract19Jun2023-30Jun2023.csv\n",
      "Downloaded 376JourneyDataExtract01Jul2023-14Jul2023.csv\n",
      "Downloaded 377JourneyDataExtract15Jul2023-31Jul2023.csv\n",
      "Downloaded 378JourneyDataExtract01Aug2023-14Aug2023.csv\n",
      "Downloaded 378JourneyDataExtract15Aug2023-31Aug2023.csv\n",
      "Downloaded 379JourneyDataExtract01Sep2023-14Sep2023.csv\n",
      "Downloaded 380JourneyDataExtract15Sep2023-30Sep2023.csv\n",
      "Downloaded 381JourneyDataExtract01Oct2023-14Oct2023.csv\n",
      "Downloaded 382JourneyDataExtract15Oct2023-31Oct2023.csv\n",
      "Downloaded 383JourneyDataExtract01Nov2023-14Nov2023.csv\n",
      "Downloaded 384JourneyDataExtract15Nov2023-30Nov2023.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Directory where you want to save the files\n",
    "save_dir = 'downloaded_files'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "for url in url_list:\n",
    "    # Extracting the file name from the URL\n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Making the request and saving the file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de838667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nc/3_jdz57s3fb248nc5jbmskj40000gn/T/ipykernel_67595/1727438051.py:12: DtypeWarning: Columns (2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/nc/3_jdz57s3fb248nc5jbmskj40000gn/T/ipykernel_67595/1727438051.py:12: DtypeWarning: Columns (2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8090413, 11)\n",
      "      Number        Start date Start station number  \\\n",
      "0  134489498  2023-09-30 23:59                 1083   \n",
      "1  134489510  2023-09-30 23:59                 3504   \n",
      "2  134489509  2023-09-30 23:59                 3504   \n",
      "3  134489508  2023-10-01 00:00                 1133   \n",
      "4  134489507  2023-09-30 23:59               300079   \n",
      "\n",
      "                   Start station          End date End station number  \\\n",
      "0  Commercial Street, Shoreditch  2023-10-01 00:08             200121   \n",
      "1              Moor Street, Soho  2023-10-01 01:02             300234   \n",
      "2              Moor Street, Soho  2023-10-01 01:04             300234   \n",
      "3          Baylis Road, Waterloo  2023-10-01 00:06               1078   \n",
      "4      London Street, Paddington  2023-10-01 00:16               1214   \n",
      "\n",
      "                           End station  Bike number Bike model Total duration  \\\n",
      "0         Shoreditch Court, Haggerston        57640    CLASSIC          9m 1s   \n",
      "1         Stratford Station, Stratford        57335    CLASSIC      1h 2m 59s   \n",
      "2         Stratford Station, Stratford        30035    CLASSIC      1h 4m 27s   \n",
      "3               Lambeth Road, Vauxhall        52136    CLASSIC         5m 59s   \n",
      "4  Kensington Olympia Station, Olympia        57534    CLASSIC        16m 29s   \n",
      "\n",
      "   Total duration (ms)  \n",
      "0               541565  \n",
      "1              3779551  \n",
      "2              3867026  \n",
      "3               359570  \n",
      "4               989750  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Start date</th>\n",
       "      <th>Start station number</th>\n",
       "      <th>Start station</th>\n",
       "      <th>End date</th>\n",
       "      <th>End station number</th>\n",
       "      <th>End station</th>\n",
       "      <th>Bike number</th>\n",
       "      <th>Bike model</th>\n",
       "      <th>Total duration</th>\n",
       "      <th>Total duration (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134489498</td>\n",
       "      <td>2023-09-30 23:59</td>\n",
       "      <td>1083</td>\n",
       "      <td>Commercial Street, Shoreditch</td>\n",
       "      <td>2023-10-01 00:08</td>\n",
       "      <td>200121</td>\n",
       "      <td>Shoreditch Court, Haggerston</td>\n",
       "      <td>57640</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>9m 1s</td>\n",
       "      <td>541565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134489510</td>\n",
       "      <td>2023-09-30 23:59</td>\n",
       "      <td>3504</td>\n",
       "      <td>Moor Street, Soho</td>\n",
       "      <td>2023-10-01 01:02</td>\n",
       "      <td>300234</td>\n",
       "      <td>Stratford Station, Stratford</td>\n",
       "      <td>57335</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>1h 2m 59s</td>\n",
       "      <td>3779551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134489509</td>\n",
       "      <td>2023-09-30 23:59</td>\n",
       "      <td>3504</td>\n",
       "      <td>Moor Street, Soho</td>\n",
       "      <td>2023-10-01 01:04</td>\n",
       "      <td>300234</td>\n",
       "      <td>Stratford Station, Stratford</td>\n",
       "      <td>30035</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>1h 4m 27s</td>\n",
       "      <td>3867026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134489508</td>\n",
       "      <td>2023-10-01 00:00</td>\n",
       "      <td>1133</td>\n",
       "      <td>Baylis Road, Waterloo</td>\n",
       "      <td>2023-10-01 00:06</td>\n",
       "      <td>1078</td>\n",
       "      <td>Lambeth Road, Vauxhall</td>\n",
       "      <td>52136</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>5m 59s</td>\n",
       "      <td>359570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134489507</td>\n",
       "      <td>2023-09-30 23:59</td>\n",
       "      <td>300079</td>\n",
       "      <td>London Street, Paddington</td>\n",
       "      <td>2023-10-01 00:16</td>\n",
       "      <td>1214</td>\n",
       "      <td>Kensington Olympia Station, Olympia</td>\n",
       "      <td>57534</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>16m 29s</td>\n",
       "      <td>989750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090408</th>\n",
       "      <td>131002744</td>\n",
       "      <td>2023-05-29 00:00</td>\n",
       "      <td>2685</td>\n",
       "      <td>South Kensington Station, South Kensington</td>\n",
       "      <td>2023-05-29 00:03</td>\n",
       "      <td>3461</td>\n",
       "      <td>Sloane Avenue, Knightsbridge</td>\n",
       "      <td>51118</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>2m 52s</td>\n",
       "      <td>172684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090409</th>\n",
       "      <td>131002745</td>\n",
       "      <td>2023-05-29 00:00</td>\n",
       "      <td>22179</td>\n",
       "      <td>Exhibition Road, Knightsbridge</td>\n",
       "      <td>2023-05-29 00:14</td>\n",
       "      <td>200142</td>\n",
       "      <td>Hammersmith Road, Hammersmith</td>\n",
       "      <td>55736</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>14m 26s</td>\n",
       "      <td>866745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090410</th>\n",
       "      <td>131002746</td>\n",
       "      <td>2023-05-29 00:00</td>\n",
       "      <td>1190</td>\n",
       "      <td>Kennington Lane Rail Bridge, Vauxhall</td>\n",
       "      <td>2023-05-29 00:03</td>\n",
       "      <td>1205</td>\n",
       "      <td>Black Prince Road, Vauxhall</td>\n",
       "      <td>54947</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>3m 7s</td>\n",
       "      <td>187160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090411</th>\n",
       "      <td>131002747</td>\n",
       "      <td>2023-05-29 00:00</td>\n",
       "      <td>200152</td>\n",
       "      <td>Pitfield Street North,Hoxton</td>\n",
       "      <td>2023-05-29 00:04</td>\n",
       "      <td>200246</td>\n",
       "      <td>Eagle Wharf Road, Hoxton</td>\n",
       "      <td>35110</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>4m 36s</td>\n",
       "      <td>276002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090412</th>\n",
       "      <td>131002748</td>\n",
       "      <td>2023-05-29 00:00</td>\n",
       "      <td>1028</td>\n",
       "      <td>William IV Street, Strand</td>\n",
       "      <td>2023-05-29 00:24</td>\n",
       "      <td>200216</td>\n",
       "      <td>Clements Road, Bermondsey</td>\n",
       "      <td>56642</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>24m 2s</td>\n",
       "      <td>1442965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8090413 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Number        Start date Start station number  \\\n",
       "0        134489498  2023-09-30 23:59                 1083   \n",
       "1        134489510  2023-09-30 23:59                 3504   \n",
       "2        134489509  2023-09-30 23:59                 3504   \n",
       "3        134489508  2023-10-01 00:00                 1133   \n",
       "4        134489507  2023-09-30 23:59               300079   \n",
       "...            ...               ...                  ...   \n",
       "8090408  131002744  2023-05-29 00:00                 2685   \n",
       "8090409  131002745  2023-05-29 00:00                22179   \n",
       "8090410  131002746  2023-05-29 00:00                 1190   \n",
       "8090411  131002747  2023-05-29 00:00               200152   \n",
       "8090412  131002748  2023-05-29 00:00                 1028   \n",
       "\n",
       "                                      Start station          End date  \\\n",
       "0                     Commercial Street, Shoreditch  2023-10-01 00:08   \n",
       "1                                 Moor Street, Soho  2023-10-01 01:02   \n",
       "2                                 Moor Street, Soho  2023-10-01 01:04   \n",
       "3                             Baylis Road, Waterloo  2023-10-01 00:06   \n",
       "4                         London Street, Paddington  2023-10-01 00:16   \n",
       "...                                             ...               ...   \n",
       "8090408  South Kensington Station, South Kensington  2023-05-29 00:03   \n",
       "8090409              Exhibition Road, Knightsbridge  2023-05-29 00:14   \n",
       "8090410       Kennington Lane Rail Bridge, Vauxhall  2023-05-29 00:03   \n",
       "8090411                Pitfield Street North,Hoxton  2023-05-29 00:04   \n",
       "8090412                   William IV Street, Strand  2023-05-29 00:24   \n",
       "\n",
       "        End station number                          End station  Bike number  \\\n",
       "0                   200121         Shoreditch Court, Haggerston        57640   \n",
       "1                   300234         Stratford Station, Stratford        57335   \n",
       "2                   300234         Stratford Station, Stratford        30035   \n",
       "3                     1078               Lambeth Road, Vauxhall        52136   \n",
       "4                     1214  Kensington Olympia Station, Olympia        57534   \n",
       "...                    ...                                  ...          ...   \n",
       "8090408               3461         Sloane Avenue, Knightsbridge        51118   \n",
       "8090409             200142        Hammersmith Road, Hammersmith        55736   \n",
       "8090410               1205          Black Prince Road, Vauxhall        54947   \n",
       "8090411             200246             Eagle Wharf Road, Hoxton        35110   \n",
       "8090412             200216            Clements Road, Bermondsey        56642   \n",
       "\n",
       "        Bike model Total duration  Total duration (ms)  \n",
       "0          CLASSIC          9m 1s               541565  \n",
       "1          CLASSIC      1h 2m 59s              3779551  \n",
       "2          CLASSIC      1h 4m 27s              3867026  \n",
       "3          CLASSIC         5m 59s               359570  \n",
       "4          CLASSIC        16m 29s               989750  \n",
       "...            ...            ...                  ...  \n",
       "8090408    CLASSIC         2m 52s               172684  \n",
       "8090409    CLASSIC        14m 26s               866745  \n",
       "8090410    CLASSIC          3m 7s               187160  \n",
       "8090411    CLASSIC         4m 36s               276002  \n",
       "8090412    CLASSIC         24m 2s              1442965  \n",
       "\n",
       "[8090413 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory where the files were saved\n",
    "save_dir = 'downloaded_files'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "file_list = [f for f in os.listdir(save_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Create a DataFrame for each file and store in a list\n",
    "dataframes = []\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(save_dir, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "if dataframes:\n",
    "    london_data = pd.concat(dataframes, ignore_index=True)\n",
    "    print(london_data.shape)\n",
    "    print(london_data.head())\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")\n",
    "\n",
    "london_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8de4b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_data.to_csv('london_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dac37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
